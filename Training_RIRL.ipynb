{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2n9-bt2loEc",
        "outputId": "1449df91-ad0b-4398-82dd-c1879ee94c8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'CLVR_Impl_RIRL'...\n",
            "remote: Enumerating objects: 228, done.\u001b[K\n",
            "remote: Counting objects: 100% (228/228), done.\u001b[K\n",
            "remote: Compressing objects: 100% (162/162), done.\u001b[K\n",
            "remote: Total 228 (delta 84), reused 206 (delta 62), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (228/228), 70.12 MiB | 21.82 MiB/s, done.\n",
            "Resolving deltas: 100% (84/84), done.\n",
            "Updating files: 100% (125/125), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/jellyho/CLVR_Impl_RIRL.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaezbpIrn61y",
        "outputId": "5d53944d-1bf8-417e-d946-03f3666f35ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/CLVR_Impl_RIRL\n"
          ]
        }
      ],
      "source": [
        "%cd CLVR_Impl_RIRL/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grtC2Atwl1a-",
        "outputId": "b7d09bba-825b-4285-f2ce-03bedd029b43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment Sprites-v0 is out of date. You should consider upgrading to version `v2`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:542: UserWarning: \u001b[33mWARN: Overriding environment Sprites-v0\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {spec.id}\")\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:542: UserWarning: \u001b[33mWARN: Overriding environment Sprites-v1\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {spec.id}\")\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:542: UserWarning: \u001b[33mWARN: Overriding environment Sprites-v2\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {spec.id}\")\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:542: UserWarning: \u001b[33mWARN: Overriding environment SpritesState-v0\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {spec.id}\")\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:542: UserWarning: \u001b[33mWARN: Overriding environment SpritesState-v1\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {spec.id}\")\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:542: UserWarning: \u001b[33mWARN: Overriding environment SpritesState-v2\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {spec.id}\")\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:653: UserWarning: \u001b[33mWARN: The environment is being initialised with mode (human) that is not in the possible render_modes ([]).\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (64, 64)\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:137: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting a numpy array, actual type: <class 'list'>\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/spaces/box.py:226: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
            "  logger.warn(\"Casting input x to numpy array.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
            "  logger.warn(f\"{pre} is not within the observation space.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:137: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting a numpy array, actual type: <class 'list'>\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
            "  logger.warn(f\"{pre} is not within the observation space.\")\n",
            "Episode 1100, Total Reward: 28.416644846985697, QLoss-0.06960383303463459, ALoss--2.023917338848114\n",
            "Episode 1200, Total Reward: 28.26840757628765, QLoss-0.0781334625929594, ALoss--2.427917618751526\n",
            "Episode 1300, Total Reward: 28.35731020490232, QLoss-0.12964597884565593, ALoss--2.819076247215271\n",
            "Episode 1400, Total Reward: 28.905178212970444, QLoss-0.22728783503174782, ALoss--3.2481666517257692\n",
            "Episode 1500, Total Reward: 28.448242025920063, QLoss-0.3326465755701065, ALoss--3.710265803337097\n",
            "Episode 1600, Total Reward: 29.307551642304357, QLoss-0.5006718100607395, ALoss--4.186709327697754\n",
            "Episode 1700, Total Reward: 29.108504034704623, QLoss-0.5600086861848831, ALoss--4.696253252029419\n",
            "Episode 1800, Total Reward: 28.6576701226968, QLoss-0.8406245636940003, ALoss--5.160866022109985\n",
            "Episode 1900, Total Reward: 29.294623207353794, QLoss-1.0695530915260314, ALoss--5.668752899169922\n",
            "Episode 2000, Total Reward: 29.786469648562605, QLoss-1.1342476889491082, ALoss--6.165317678451538\n",
            "Episode 2100, Total Reward: 29.756741008239455, QLoss-1.5731946128606795, ALoss--6.651483173370361\n",
            "Episode 2200, Total Reward: 30.15038858082976, QLoss-2.001798405647278, ALoss--7.135601835250855\n",
            "Episode 2300, Total Reward: 30.307391485385434, QLoss-2.4085842108726503, ALoss--7.6102261638641355\n",
            "Episode 2400, Total Reward: 30.46709222692955, QLoss-2.58181849360466, ALoss--8.117632217407227\n",
            "Episode 2500, Total Reward: 30.51294889690406, QLoss-2.9438402915000914, ALoss--8.616618461608887\n",
            "Episode 2600, Total Reward: 31.50008168408299, QLoss-3.3430368185043333, ALoss--9.08457836151123\n",
            "Episode 2700, Total Reward: 30.449408538299643, QLoss-3.525715354681015, ALoss--9.565900917053222\n",
            "Episode 2800, Total Reward: 30.851514980089515, QLoss-3.9438416957855225, ALoss--10.04222246170044\n",
            "Episode 2900, Total Reward: 31.00036015883186, QLoss-4.887582461833954, ALoss--10.472379341125489\n",
            "Episode 3000, Total Reward: 31.12252695802557, QLoss-5.406236290931702, ALoss--10.934297428131103\n",
            "Episode 3100, Total Reward: 31.2168432116198, QLoss-5.3545134115219115, ALoss--11.409199981689452\n",
            "Episode 3200, Total Reward: 31.573691998102014, QLoss-6.62421597957611, ALoss--11.82931381225586\n",
            "Episode 3300, Total Reward: 31.501652112343447, QLoss-6.502125029563904, ALoss--12.324538707733154\n",
            "Episode 3400, Total Reward: 31.107776584282814, QLoss-6.913344211578369, ALoss--12.703554039001466\n",
            "Episode 3500, Total Reward: 31.54391926544348, QLoss-7.611256079673767, ALoss--13.147828216552734\n",
            "Episode 3600, Total Reward: 31.21051455843529, QLoss-8.127598600387573, ALoss--13.541581344604491\n",
            "Episode 3700, Total Reward: 31.71052145684306, QLoss-8.219198112487794, ALoss--13.970777626037597\n",
            "Episode 3800, Total Reward: 31.442730380597396, QLoss-8.376676228046417, ALoss--14.361144256591796\n",
            "Episode 3900, Total Reward: 31.150300500275605, QLoss-9.666253433227538, ALoss--14.747698249816894\n",
            "Episode 4000, Total Reward: 31.77925653857824, QLoss-10.215122985839844, ALoss--15.117656059265137\n",
            "Episode 4100, Total Reward: 31.43676845582875, QLoss-10.200571241378784, ALoss--15.507337131500243\n",
            "Episode 4200, Total Reward: 30.875060986212215, QLoss-11.361154356002807, ALoss--15.8669234085083\n",
            "Episode 4300, Total Reward: 31.472047461347987, QLoss-12.108948254585266, ALoss--16.219711265563966\n",
            "Episode 4400, Total Reward: 30.950617493330512, QLoss-12.392845878601074, ALoss--16.574184341430666\n",
            "Episode 4500, Total Reward: 31.36537790769476, QLoss-12.80330446243286, ALoss--16.91158962249756\n",
            "Episode 4600, Total Reward: 31.17841840734102, QLoss-14.665365810394286, ALoss--17.206966972351076\n"
          ]
        }
      ],
      "source": [
        "!python train_agent.py -m encoder -t Sprites-v0 -d ./Results/agents"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
